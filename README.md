# AI Code Examples Generator and Evaluator

This project consists of two main scripts:

1. `generate_code_examples.py`: Generates coding examples for software engineering interviews using AI models
2. `evaluate_code_examples.py`: Evaluates AI models on their ability to solve these coding examples

The scripts use OpenAI and Anthropic APIs to generate and evaluate deterministic coding problems of varying difficulty levels.

## Setup

1. Install the required dependencies:
```bash
pip install openai anthropic python-dotenv
```

2. Create a `.env` file in the same directory as the scripts with your API keys:
```
OPENAI_API_KEY=your_openai_api_key
ANTHROPIC_API_KEY=your_anthropic_api_key
```

3. Make sure the output directories exist (or they will be created automatically):
```bash
mkdir -p output/examples output/evals
```

Alternatively, you can set these environment variables directly in your shell.

## Script 1: Generate Code Examples

### Usage

Run the script with the following command:

```bash
python generate_code_examples.py --provider [openai|anthropic] --model [model_name]
```

### Required Arguments

- `--provider`: The API provider to use (`openai` or `anthropic`)
- `--model`: The model name to use (required unless using `--list-models`)
  - For OpenAI: e.g., `gpt-4`, `gpt-3.5-turbo`
  - For Anthropic: e.g., `claude-3-opus-20240229`, `claude-3-sonnet-20240229`

### Optional Arguments

- `--prompt`: Custom prompt to use (defaults to the predefined prompt asking for coding examples)
- `--list-models`: List available models from the specified provider and exit
- `--dry-run`: Show what would be sent to the API without actually making the call

### Examples

Using OpenAI's GPT-4:
```bash
python generate_code_examples.py --provider openai --model gpt-4
```

Listing available OpenAI models:
```bash
python generate_code_examples.py --provider openai --list-models
```

Testing with dry run (no API call):
```bash
python generate_code_examples.py --provider openai --model gpt-4 --dry-run
```

### Output

The script saves the results to a JSON file with a timestamp in the filename, in the `output/examples` directory.

Example path: `output/examples/code_examples_openai_gpt_4_20240601_123456.json`

## Script 2: Evaluate Code Examples

### Usage

Run the script with the following command:

```bash
python evaluate_code_examples.py --input-file [path_to_json_file] --provider [openai|anthropic] --model [model_name]
```

### Required Arguments

- `--input-file`: Path to the JSON file containing code examples (generated by the first script)
- `--provider`: The API provider to use (`openai` or `anthropic`)
- `--model`: The model name to use (required unless using `--list-models`)

### Optional Arguments

- `--language`: Filter examples by language (e.g., Python, C, Javascript)
- `--list-models`: List available models from the specified provider and exit
- `--dry-run`: Show what would be sent to the API without actually making the call

### Examples

Evaluating with OpenAI's GPT-4:
```bash
python evaluate_code_examples.py --input-file output/examples/code_examples_openai_gpt_4_20240601_123456.json --provider openai --model gpt-4
```

Evaluating only Python examples:
```bash
python evaluate_code_examples.py --input-file output/examples/code_examples_openai_gpt_4_20240601_123456.json --provider openai --model gpt-4 --language Python
```

Testing with dry run (no API call):
```bash
python evaluate_code_examples.py --input-file output/examples/code_examples_openai_gpt_4_20240601_123456.json --provider openai --model gpt-4 --dry-run
```

### Output

The script saves the evaluation results to a JSON file with a timestamp in the filename, in the `output/evals` directory.

Example path: `output/evals/evaluation_openai_gpt_4_20240601_123456.json`

## Workflow

The typical workflow is:

1. Generate coding examples using the first script:
   ```bash
   python generate_code_examples.py --provider openai --model gpt-4
   ```

2. Evaluate models on those examples using the second script:
   ```bash
   python evaluate_code_examples.py --input-file output/examples/code_examples_openai_gpt_4_*.json --provider anthropic --model claude-3-opus-20240229
   ```

This allows you to:
- Generate coding examples with one model
- Test the same examples with different models
- Compare how different models perform on the same problems
- Test models on specific programming languages

## Files

- `generate_code_examples.py`: Script to generate coding examples
- `evaluate_code_examples.py`: Script to evaluate models on examples
- `ai_utils.py`: Shared utilities used by both scripts
- `requirements.txt`: Required dependencies
- `.env.example`: Template for API keys
- `output/examples/`: Directory where generated examples are stored
- `output/evals/`: Directory where evaluation results are stored
