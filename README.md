# AI Code Examples Generator and Evaluator

This project consists of three main scripts:

1. `generate_code_examples.py`: Generates coding examples for software engineering interviews using AI models
2. `evaluate_code_examples.py`: Evaluates AI models on their ability to solve these coding examples
3. `run_code_examples.py`: Verifies code examples by actually running them and comparing results

The scripts use OpenAI and Anthropic APIs to generate and evaluate deterministic coding problems of varying difficulty levels.

## Setup

1. Install the required dependencies:
```bash
pip install openai anthropic python-dotenv
```

2. Create a `.env` file in the same directory as the scripts with your API keys:
```
OPENAI_API_KEY=your_openai_api_key
ANTHROPIC_API_KEY=your_anthropic_api_key
```

3. Make sure the output directories exist (or they will be created automatically):
```bash
mkdir -p output/examples output/evals output/verified
```

4. For code verification, ensure you have the necessary language runtimes installed:
   - For JavaScript: Install Node.js
   - For Python: Use Python 3
   - For C: Install GCC

Alternatively, you can set the API environment variables directly in your shell.

## Script 1: Generate Code Examples

### Usage

Run the script with the following command:

```bash
python generate_code_examples.py --provider [openai|anthropic] --model [model_name]
```

### Required Arguments

- `--provider`: The API provider to use (`openai` or `anthropic`)
- `--model`: The model name to use (required unless using `--list-models`)
  - For OpenAI: e.g., `gpt-4`, `gpt-3.5-turbo`
  - For Anthropic: e.g., `claude-3-opus-20240229`, `claude-3-sonnet-20240229`

### Optional Arguments

- `--prompt`: Custom prompt to use (defaults to the predefined prompt asking for coding examples)
- `--list-models`: List available models from the specified provider and exit
- `--dry-run`: Show what would be sent to the API without actually making the call

### Examples

Using OpenAI's GPT-4:
```bash
python generate_code_examples.py --provider openai --model gpt-4
```

Listing available OpenAI models:
```bash
python generate_code_examples.py --provider openai --list-models
```

Testing with dry run (no API call):
```bash
python generate_code_examples.py --provider openai --model gpt-4 --dry-run
```

### Output

The script saves the results to a JSON file with a timestamp in the filename, in the `output/examples` directory.

Example path: `output/examples/code_examples_openai_gpt_4_20240601_123456.json`

## Script 2: Evaluate Code Examples

### Usage

Run the script with the following command:

```bash
python evaluate_code_examples.py --input-file [path_to_json_file] --provider [openai|anthropic] --model [model_name]
```

### Required Arguments

- `--input-file`: Path to the JSON file containing code examples (generated by the first script)
- `--provider`: The API provider to use (`openai` or `anthropic`)
- `--model`: The model name to use (required unless using `--list-models`)

### Optional Arguments

- `--language`: Filter examples by language (e.g., Python, C, Javascript)
- `--list-models`: List available models from the specified provider and exit
- `--dry-run`: Show what would be sent to the API without actually making the call

### Examples

Evaluating with OpenAI's GPT-4:
```bash
python evaluate_code_examples.py --input-file output/examples/code_examples_openai_gpt_4_20240601_123456.json --provider openai --model gpt-4
```

Evaluating only Python examples:
```bash
python evaluate_code_examples.py --input-file output/examples/code_examples_openai_gpt_4_20240601_123456.json --provider openai --model gpt-4 --language Python
```

Testing with dry run (no API call):
```bash
python evaluate_code_examples.py --input-file output/examples/code_examples_openai_gpt_4_20240601_123456.json --provider openai --model gpt-4 --dry-run
```

### Output

The script saves the evaluation results to a JSON file with a timestamp in the filename, in the `output/evals` directory.

Example path: `output/evals/evaluation_openai_gpt_4_20240601_123456.json`

## Script 3: Verify Code Examples

This script actually runs the code examples to verify their expected outputs, helping identify which examples have correct answers.

### Usage

Run the script with the following command:

```bash
python run_code_examples.py --input-file [path_to_json_file]
```

### Required Arguments

- `--input-file`: Path to the JSON file containing code examples (generated by the first script)

### Optional Arguments

- `--language`: Filter examples by language (e.g., Python, C, Javascript)
- `--verbose`: Show detailed output for each example

### Examples

Verifying all examples:
```bash
python run_code_examples.py --input-file output/examples/code_examples_openai_gpt_4_20240601_123456.json
```

Verifying only Python examples:
```bash
python run_code_examples.py --input-file output/examples/code_examples_openai_gpt_4_20240601_123456.json --language Python
```

### Output

The script saves the verification results to a JSON file in the `output/verified` directory.

Example path: `output/verified/verified_code_examples_openai_gpt_4_20240601_123456.json`

## Complete Workflow

The typical workflow is:

1. Generate coding examples using the first script:
   ```bash
   python generate_code_examples.py --provider openai --model gpt-4
   ```

2. Verify the generated examples by running them:
   ```bash
   python run_code_examples.py --input-file output/examples/code_examples_openai_gpt_4_*.json
   ```

3. Evaluate models on those examples using the second script:
   ```bash
   python evaluate_code_examples.py --input-file output/examples/code_examples_openai_gpt_4_*.json --provider anthropic --model claude-3-opus-20240229
   ```

This workflow allows you to:
- Generate coding examples with one model
- Verify which examples actually produce the expected output
- Test the same examples with different AI models
- Compare how different models perform on the same problems
- Test models on specific programming languages

## Files

- `generate_code_examples.py`: Script to generate coding examples
- `evaluate_code_examples.py`: Script to evaluate models on examples
- `run_code_examples.py`: Script to verify examples by actually running the code
- `ai_utils.py`: Shared utilities used by the scripts
- `requirements.txt`: Required dependencies
- `.env.example`: Template for API keys
- `output/examples/`: Directory where generated examples are stored
- `output/evals/`: Directory where evaluation results are stored
- `output/verified/`: Directory where verification results are stored

# Code Examples Runner

This script runs code examples in different programming languages (C, Python, JavaScript) and verifies their outputs.

## Recent Enhancements

1. **Proper C Program Handling**
   - Added automatic detection and injection of a `main()` function for C programs
   - Includes intelligent detection of function names and return types
   - Handles various C code structures and formats

2. **Verified Answers**
   - Changed from keeping original AI-provided answers to using verified results from actual code execution
   - Replaced `answer` key with `verified_answer` containing the actual output
   - Added tracking for answer changes across verification runs

3. **Debugging Support**
   - Added `--debug` flag to show detailed diagnostic information
   - Shows intermediate code generation and execution details
   - Helps troubleshoot execution problems

4. **Skip Already Verified Examples**
   - Added `--skip-verified` flag to skip examples that already have verified answers
   - Useful for large datasets where you only want to process new or changed examples

## Usage

```bash
python3 run_code_examples.py --input-file [file.json] [options]
```

### Options

- `--input-file`: Path to JSON file containing code examples (required)
- `--language`: Filter examples by language (e.g., Python, C, JavaScript)
- `--verbose`: Show detailed output for each example
- `--debug`: Show additional debugging information
- `--skip-verified`: Skip examples that already have verified answers

## JSON Format

The script expects input files in this format:

```json
{
  "examples": [
    {
      "language": "C",
      "difficulty": 5,
      "code": "int function() { ... }"
    }
  ]
}
```

After verification, it generates output with:

```json
{
  "examples": [
    {
      "language": "C",
      "difficulty": 5,
      "code": "int function() { ... }",
      "verified_answer": "25",
      "success": true
    }
  ],
  "verification_timestamp": "..."
}
```
